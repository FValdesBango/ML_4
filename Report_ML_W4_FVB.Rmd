---
title: "Report Machine Learning Week 4 "
author: "FVB"
date: "05/03/2018"
output: html_document
---

## Summary

In this report we build a machine learning algorithm that accurately predicts the fashion in which a human is performing a dumbbell lift. The prediction is done using data captured from accelerometers that measure bodily movements of subjects performing the dumbbell lifts based on study from Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Six male subjects ranging in age between 20 - 28 years were fitted with accelerometers.The subjects performed dumbbell lifts in five different fashions, one correct way (class A), and four different incorrect ways (classes B, C, D, E)

After diferent tree models a random forest model has been employed to fit the data reaching high levels of accuracy(99%) both in sample and in test expectations.

## Data Treatment

#### Data Cleaning

After downloading our data from its online source we transform all measurements values to numeric format except dates, user and classification columns.

```{r loading, echo=T, message=FALSE,warning=FALSE}
library(arm)
library(caret)
library(ggplot2)
library(tidyverse)
library(cowplot)
library(gridExtra)
library(rpart.plot)

train.raw<-read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
train.num<-data.frame(train.raw)
train.num[,c(-(1:7),-160)]<-lapply(train.num[,c(-(1:7),-160)],as.numeric)
```

Before data analysis itself we check for zero variance variables and those with more than half full of NA values in order to remove them from our future study.This process reduces the number of variables from 160 to 59.

```{r filtering, echo=T, message=FALSE,warning=FALSE}
metrics<-nearZeroVar(train.num,saveMetrics = T)
delnames<-rownames(metrics[(metrics$nzv==1),])
train.num.nozer<-train.num[,!(names(train.num) %in% delnames)]

total.na<-lapply(train.num.nozer,function(x)  {sum(is.na(x))/length(x)})
delnames2<-names(total.na[total.na>0.5])
train.no.na.zvar<-train.num.nozer[,!names(train.num.nozer)%in%delnames2]
```


We can observe that our data first variables are formed by id of observation (an index), followed by user name, three columns of measuring dates and finally window time range of measuring.

By plotting classification parameter in function of these variables (Fig.1) we can see that they followed a pattern for all users but this is clearly generated by the design of the experiment.Therefore they don't seem variables necessary to be taken in account in order to predict future values.

```{r filtering_plot, echo=F, message=FALSE, fig.align="centre",fig.width=10}
plot.v.dates<-ggplot(data = train.no.na.zvar, aes(x=classe,y=cvtd_timestamp,col=user_name))+geom_point()

plot.w.dates<-ggplot(data = train.no.na.zvar, aes(x=classe,y=num_window,col=user_name))+geom_point()

plot_grid(plot.v.dates, plot.w.dates, labels = c("Classe Vs Date", "Classe vs Window Range"))
```

*Fig.1: Plot of class category in function of time variables*

#### Cross Validation Partition

Due to sample size we can make partition fo our observatiosn for future predictions
```{r cross, echo=T, message=FALSE,warning=FALSE}
train.cut<-train.no.na.zvar[,-c(1,3,4,5,6)]

set.seed(333)
real.train <-createDataPartition(y = train.cut$classe, p = 0.75, list = FALSE)
train.clear <- train.cut[real.train, ]
train.testcv <- train.cut[-real.train, ]
```

#### Exploratory Analysis

The last thing we can do to improve algorithm performance is to eliminate variables that are redundant: We analyse  the correlation of the variables. By setting a threshold of 0.88 in the level of correlation we can identify potentially repeated information. We can study in more depth some of these relationships to be sure it makes sense to eliminate them (Fig.2).This reduce the number of variables from 54 to 47 (45 if we exclude user id and classification).

```{r reduce_var, echo=T, message=FALSE,warning=FALSE}


var.cor.mat<-cor(x=train.clear[,c(-1,-54)])
diag(var.cor.mat)<-0

flattenCorrMatrix <- function(cormat) {
        ut <- upper.tri(cormat)
        data.frame(
                row = rownames(cormat)[row(cormat)[ut]],
                column = rownames(cormat)[col(cormat)[ut]],
                cor  =(cormat)[ut]
        )
}


cor.list<-flattenCorrMatrix(var.cor.mat)

high.cor<-cor.list[abs(cor.list$cor)>0.88,]

high.cor

names.correlated<-c("total_accel_belt","accel_belt_y","accel_belt_z","accel_belt_x",
                    "magnet_belt_x","gyros_dumbbell_x","gyros_forearm_z")

train.clear.filtered<-train.clear[,!(names(train.clear) %in% names.correlated)]
```

If we plot class in function of the rest of variables we can observe some of them showing clear distinct patterns for each class (particulary A and C, Fig.2), inducing the idea that we could roughly predict based on them.

```{r exploration_var, echo=F, message=FALSE,warning=FALSE,fig.height=6,fig.width=10}



main.var.simple<-c("roll_belt","pitch_belt","yaw_belt","roll_arm","roll_dumbbell",
                   "pitch_dumbbell","yaw_dumbbell","roll_forearm","pitch_forearm",
                   "yaw_forearm")
plot.rb<-ggplot(data = train.clear, aes(x=classe,y=roll_belt))+geom_boxplot()
plot.pb<-ggplot(data = train.clear, aes(x=classe,y=pitch_belt))+geom_boxplot()
plot.yb<-ggplot(data = train.clear, aes(x=classe,y=yaw_belt))+geom_boxplot()
plot.ra<-ggplot(data = train.clear, aes(x=classe,y=roll_arm))+geom_boxplot()
plot.rd<-ggplot(data = train.clear, aes(x=classe,y=roll_dumbbell))+geom_boxplot()
plot.pd<-ggplot(data = train.clear, aes(x=classe,y=pitch_dumbbell))+geom_boxplot()
plot.yd<-ggplot(data = train.clear, aes(x=classe,y=yaw_dumbbell))+geom_boxplot()
plot.rf<-ggplot(data = train.clear, aes(x=classe,y=roll_forearm))+geom_boxplot()
plot.pf<-ggplot(data = train.clear, aes(x=classe,y=pitch_forearm))+geom_boxplot()
plot.yf<-ggplot(data = train.clear, aes(x=classe,y=yaw_forearm))+geom_boxplot()

grid.arrange(plot.rb, plot.pb, plot.yb, plot.ra, plot.rd, plot.pd, plot.yd, plot.rf, plot.pf, plot.yf, ncol = 3, nrow = 4)

```
*Fig.2: Plot of those variables with the main differenciated distributions by class*

#### Modelling

We could start by trying a simple forest model on these 10 variables  variables.We compare the results with an analogous model with all the 45 variables.

We can observe that filtering these variables actually improves the prediction of our model over a tree model with the whole set (0.58 vs 0.51).

```{r models_1, eval=T ,echo=T, message=FALSE,warning=FALSE,cache=T}

set.seed(333)
model.t.simple<-train(data=train.clear.filtered[,c(which(names(train.clear.filtered)%in%main.var.simple),47)],classe~.,method="rpart")

set.seed(333)
model.t.full<-train(data=train.clear.filtered[,-1],classe~.,method="rpart")

rpart.plot(model.t.simple$finalModel)
```

We can further improve our performance trying a random forest model. Where we obtain an accuracy of 0.99 (identical independently for K-fold higher than 2) 


```{r models_2, eval=T ,echo=T, message=FALSE,warning=FALSE, cache=T}
set.seed(333)
tr.ct<-trainControl(method = "cv", number = 2)
model.rf.n2<-train(data=train.clear.filtered[,-1],classe~.,method="rf",trControl=tr.ct)

```

#### Error Estimation.

We can evaluate our different models in our created test set to obtain a prediction of the error for our of training samples, obtaining similar values to those observed in the model train sample.

```{r error_esti}
pred.simp <- predict(model.t.simple, newdata = train.testcv[,])
pred.rf <- predict(model.rf.n2, newdata = train.testcv[,])

confusionMatrix(pred.simp, train.testcv$classe)
confusionMatrix(pred.rf, train.testcv$classe)

```

## Conclusions

A random forest model seems to be the best choice with an accuracy of approximately 99%. As an interesting fact,if we focus ourselves on the weight of variables in the models we will see that form the 10 most relevant variables half of them were already in our simple  tree model.

```{r weights, echo=F}
weight.var<-varImp(model.rf.n2)
 plot(weight.var)
```

*Fig.3: Variables weight in random forest*